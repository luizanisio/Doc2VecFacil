# Doc2VecFacil
Componente python que simplifica o processo de cria√ß√£o de um modelo `Doc2Vec` [`Gensim 4.0.1`](https://radimrehurek.com/gensim/models/doc2vec.html) com facilitadores para gera√ß√£o de um vocab personalizado e com a gera√ß√£o de arquivos de curadoria. Dicas de agrupamento de documentos similares, uso de `ElasticSearch` e `SingleStore`.
- se voc√™ n√£o sabe o que √© um modelo de similaridade, em resumo √© um algoritmo n√£o supervisionado para criar um modelo que transforma frases ou documentos em vetores matem√°ticos que podem ser comparados retornando um valor equivalente √† similaridade sem√¢ntica de documentos do mesmo contexto/dom√≠nio dos documentos usados no treinamento do modelo (doc2vec). Nesse cen√°rio a m√°quina 'aprende' o vocabul√°rio treinado e o contexto em que as palavras aparecem (word2vec), permitindo identificar a similaridade entre os termos, as frases e os documentos. O doc2vec amplia o treinamento do word2vec para frases ou documentos.
  - alguns links para saber mais: [`Paragraph Vector 2014`](https://cs.stanford.edu/~quocle/paragraph_vector.pdf), [`me Amilar 2018`](https://repositorio.idp.edu.br/handle/123456789/2635), [`Representa√ß√£o Sem√¢ntica Vetorial`](https://sol.sbc.org.br/index.php/erigo/article/view/7125), [`Word2Vec Explained`](https://www.youtube.com/watch?v=yFFp9RYpOb0), [`Word Embedding Explained and Visualized`](https://www.youtube.com/watch?v=D-ekE-Wlcds), [`Gensim 4.0.1 Doc2Vec`](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html), [`ti-exame`](https://www.ti-enxame.com/pt/python/como-calcular-similaridade-de-sentenca-usando-o-modelo-word2vec-de-gensim-com-python/1045257495/), [`Tomas Mikolov paper`](https://arxiv.org/pdf/1301.3781.pdf).
- Com essa compara√ß√£o vetorial, √© poss√≠vel encontrar documentos semelhantes a um indicado, [`agrupar documentos semelhantes`](./docs/agrupamento.md) entre si de uma lista de documentos, e tamb√©m monitorar documentos que entram na base ao compar√°-los com os documentos rotulados para monitoramento, como uma classifica√ß√£o r√°pida. Pode-se armazenar os vetores no `SingleStore` ou `Elasticsearch` como nas dicas ao final deste documento [aqui](#dicas).
- Em um recorte do espa√ßo vetorial criado pelo treinamento do modelo, pode-se perceber que documentos semelhantes ficam pr√≥ximos enquanto documentos diferentes ficam distantes entre sim. Ent√£o agrupar ou buscar documentos semelhantes √© uma quest√£o de identificar a dist√¢ncia vetorial dos documentos ap√≥s o treinamento. Armazenando os vetores dos documento no `ElasticSearch` ou `SingleStore`, essa tarefa √© simplificada, permitindo construir sistemas de busca sem√¢ntica com um esfor√ßo pequeno.
![exemplo recorte espa√ßo vetorial](./exemplos/img_recorte_espaco_vetorial.png?raw=true "Exemplo recorte de espa√ßo vetorial")

- O core desse componente √© o uso de um Tokenizador Inteligente que usa as configura√ß√µes dos arquivos contidos na pasta do modelo para tokenizar os arquivos de treinamento e os arquivos novos para compara√ß√£o no futuro (toda a configura√ß√£o do tokenizador √© opcional).
- Esse √© um reposit√≥rio de estudos. Analise, ajuste, corrija e use os c√≥digos como desejar.
> :thumbsup: <sub> Agradecimentos especiais ao Miguel Angelo Neto do Paran√° por v√°rios feedbacks contribuindo para a corre√ß√£o de bugs e a melhoria da documenta√ß√£o.</sub><br>

> :warning: <sub>A quantidade de termos treinados e de √©pocas de treinamento s√£o valores que dependem do objetivo e do tipo de texto de cada projeto. Quanto mais termos, mais detalhes e mais diferen√ßas ser√£o destacadas entre os textos e o modelo vai real√ßar as particularidades da escrita. Escolhendo menos termos mais relacionados com o dom√≠nio analisado, e compondo ngramas, h√° uma chance maior do modelo real√ßar a tem√°tica geral dos textos.</sub>

### Esse componente `Doc2VecFacil` trabalha em duas etapas:
 - cria√ß√£o/configura√ß√£o e curadoria de um [vocab](#vocab) personalizado para o [Tokenizador Inteligente](#tokenizador).
   - `python util_doc2vec_vocab_facil.py -pasta ./meu_modelo`
 - treinamento do modelo usando a estrutura de tokeniza√ß√£o criada 
   - `python util_doc2vec_facil.py -pasta ./meu_modelo -treinar`
> üí° <sub>Nota: para interromper o treino sem correr o risco corromper o modelo durante a grava√ß√£o, basta criar um arquivo `meu_modelo/doc2vecfacil/parar.txt` na pasta do modelo que o treinamento ser√° interrompido ao final da itera√ß√£o em andamento.</sub>

 - Aqui tem um [`passo a passo simplificado`](./docs/passo_a_passo_facil.md) para alguns cen√°rios de treinamento com a estrutura de pastas e arquivos.
 - Alguns cen√°rios:
   - [Treino simples sem prepara√ß√£o de vocab](./docs/passo_a_passo_facil.md#1-quero-treinar-sem-preparar-um-vocab)
   - [Treino com ngramas](./docs/passo_a_passo_facil.md#3-quero-criar-ngramas-ou-limpar-o-texto-com-termos-que-n%C3%A3o-devem-ser-treinados)
   - [Treino com curadoria de termos](./docs/passo_a_passo_facil.md#4-quero-criar-meu-vocab-do-zero-fazer-curadoria-e-depois-treinar)
 
 - Logo abaixo est√£o as explica√ß√µes detalhadas de como ele funciona e como usar o seu modelo para pesquisas de documentos semelhantes semanticamente (por vetores) e/ou textualmente (por termos), como realizar agrupamento de documentos por similaridade para auxiliar na organiza√ß√£o de documentos usando o ElasticSearch e a pesquisa vetorial.

- :page_with_curl: <b>C√≥digos</b>: 
  - [`Cria√ß√£o de vocab`](./src/util_doc2vec_vocab_facil.py)
  - [`UtilDoc2VecFacil`](./src/util_doc2vec_facil.py) e [`UtilDoc2VecFacil_Treinamento`](./src/util_doc2vec_facil.py) 
  - [`TradutorTermos`](./src/util_tradutor_termos.py)
  - [`Cria√ß√£o de ngramas`](./src/util_ngramas_facil.py) dicas aqui [NGramasFacil](./docs/readme_ngramas.md)
  - [`UtilAgrupamentoFacil`](./src/util_agrupamento_facil.py) dicas aqui [Agrupamento](./docs/agrupamento.md)

`EM BREVE`: Ser√° disponibilizado um servi√ßo exemplo em conjunto com o componente [PesquisaElasticFacil](https://github.com/luizanisio/PesquisaElasticFacil) para cria√ß√£o de modelos de similaridade textual, agregando valor √†s pesquisas do ElasticSearch de forma simples com um modelo treinado no corpus espec√≠fico de cada projeto.

## Cria√ß√£o do vocab personalizado <a name="vocab">

O arquivo `util_doc2vec_vocab_facil.py` √© complementar √† classe `Doc2VecFacil` e serve para facilitar a cria√ß√£o de arquivos que configuram o `TokenizadorInteligente`. A ideia √© trabalhar com termos importantes para o modelo, adicionados a termos complementares compostos por fragmentos de termos `stemmer` + `sufixo`. Com isso novos documentos que possuam termos fora do vocab principal podem ter o stemmer e o sufixo dentro do vocab do modelo, criando um vocab mais flex√≠vel e menor. √â poss√≠vel tamb√©m transformar termos ou conjunto de termos durante o processamento, como criar n-gramas, reduzir nomes de organiza√ß√µes em sigla, remover termos simples ou compostos etc.

## Como funciona o Tokenizador Inteligente <a name="tokenizador">
  - Ao ser instanciado, o tokenizador busca os termos do vocab de treinamento contidos nos arquivos com padr√£o `VOCAB_BASE*.txt` (n√£o importa o case).
  - Voc√™ pode criar listas de termos que ser√£o exclu√≠dos do treinamento, basta estarem em arquivos com o padr√£o `VOCAB_REMOVIDO*.txt` - aqui geralmente ficam 'stopwords' ou termos que n√£o ajudam na diferencia√ß√£o dos documentos. Os termos contidos na lista de termos removidos n√£o s√£o fragmentados durante a tokeniza√ß√£o.
  - H√° um singularizador autom√°tico de termos. Dependendo da termina√ß√£o do termo, ele ser√° singularizado por regras simples de singulariza√ß√£o - m√©todo `singularizar( )` - e caso o resultado desse processamento esteja no vocab, o termo ficar√° no singular. Funciona para termos compostos tamb√©m. A ideia √© reduzir um pouco mais o vocab nesse passo. 
    - Exemplo: o termo `humanos` ser√° convertido em `humano` se o termo `humano` estiver no vocab. Termos no singular constantes no `VOCAB_REMOVIDO` fazem com que os termos do plural seram removidos tamb√©m. O resultado de todos os termos removidos pode ser conferido em `vocab_removido.log` gerado ao carregar o tokenizador ou o modelo.
  - Podem existir transformadores de termos nos arquivos com o padr√£o `VOCAB_TRADUTOR*.txt` que podem conter termos simples ou compostos que ser√£o convertidos em outros termos simples ou compostos, como ngramas por exempo. Veja [`NGramasFacil`](./docs/readme_ngramas.md) para mais detalhes.
  - Os tradutores funcionam antes da verifica√ß√£o dos termos do vocab. Todos os conjuntos de transforma√ß√£o s√£o inclu√≠dos automaticamente no vocab. Exemplo de configura√ß√£o:
    - `termo1 => termo2` - converte o `termo1` em `termo2` quando encontrado no texto (ex. `min => ministro`)
    - `termo1 termo2 => termo1_termo2` - converte o termo composto `termo1 termo2` em um termo √∫nico `termo1_termo2` (Ex. `processo penal => processo_penal`)
    - `termo1 termo2` - remove o termo composto `termo1 termo2` (Ex. `documento digital => ` ou `documento digital`)
  - Os tradutores podem ser usados para converter nomes de organiza√ß√µes em suas siglas, termos compostos em um termo √∫nicos (ngramas), corre√ß√µes ortogr√°ficas de termos importantes, e at√© termos conhecidos como id√™nticos convertendo para a sua forma mais usual. √â importante ressaltar que quanto maior o n√∫mero de termos para transforma√ß√£o, maior o tempo de processamento. A transforma√ß√£o e limpeza ocorrem apenas uma vez e um arquivo `.clr` √© criado para cada arquivo `.txt` com o documento pronto para treinamento.
    - Est√° dispon√≠vel um gerador de bigramas e quadrigramas aqui [`NGramasFacil`](./docs/readme_ngramas.md) para gerar sugest√µes autom√°ticas de termos que podem ser unificados.
  
> üí° <sub>A ideia de criar v√°rios arquivos √© para organizar por dom√≠nios. Pode-se, por exemplo, criar um arquivo `VOCAB_BASE portugues.txt` com termos que far√£o parte de v√°rios modelos, um arquivo `VOCAB_BASE direito.txt` com termos do direito que ser√£o somados ao primeiro no treinamento, um arquivo `VOCAB_BASE direito fragmentos.txt` com fragmentos (`stemmer` + `sufixos`) de termos do direito, e assim por diante. Facilitando evolu√ß√µes futuras dos vocabul√°rios.</sub><br>
  <sub>Pode-se realizar o treinamento sem nenhuma configura√ß√£o de vocab, ent√£o todos os termos ser√£o treinados. A ideia de criar um vocab √© poder ter algum controle do que ser√° treinado e limpar textos ocerizados que possuem muitos erros e podem aumentar muito o vocab de treinamento com ru√≠dos.</sub>

## Arquivo de curadoria para cria√ß√£o do vocab
 Ao rodar o c√≥digo `python util_doc2vec_vocab_facil.py -pasta ./meu_modelo`, ser√° criado um arquivo de curadoria de termos `curadoria_planilha_vocab.xlsx` com os termos encontrados nos textos da pasta `textos_vocab`. 
 - o objetivo do arquivo de curadoria √© permitir identificar os termos que ser√£o treinados e a import√¢ncia deles nos documentos. Pode-se interferir no treinamento retirando termos ou indicando os termos que ser√£o treinados. Pode-se realizar um treinamento com todos os termos dos documentos, nesse caso n√£o ha necessidade de identificar termos que dever√£o entrar ou sair do treinamento.
 - coloque na pasta `textos_vocab` textos que contenham boas palavras, limpas de prefer√™ncia. Podem ser listas retiradas de algum documento, n√£o importa o contexto delas, apenas as palavras nessa primeira etapa. Ent√£o listas de palavras e documentos como gram√°ticas e dicion√°rios de portugu√™s digitais parecem uma boa op√ß√£o. Coloque tamb√©m documentos com palavras relacionadas ao corpus desejado (psicologia, medicina, legisla√ß√£o, administra√ß√£o, etc). Esse site permite uma an√°lise muito boa de termos e suas caracter√≠sticas, bem como encontrar novos termos derivados de outros termos [`Dicio`](https://www.dicio.com.br/), aqui temos [`Leis`](http://www4.planalto.gov.br/legislacao/portal-legis/legislacao-1/codigos-1) e documentos Jur√≠dicos nos sites do [`STF`](https://www.stf.jus.br) ou [`STJ`](https://www.stj.jus.br).
 - Alguns termos podem n√£o ser t√£o importantes para o dom√≠nio escolhido, mas podem ser importantes para o contexto. Esses termos podem compor o dicion√°rio em forma de `stemmer` + `sufixo`. Aos termos n√£o encontrados no dicion√°rio durante a tokeniza√ß√£o para treinamento, ser√° aplicado o [`stemmer`](https://www.nltk.org/_modules/nltk/stem/snowball.html)  com o sufixo ap√≥s o stemmer. Caso o stemmer esteja no vocab de treinamento, este ser√° usado. O sufixo √© opcional e ser√° inclu√≠do se estiver no vocab e se o prefixo estiver no vocab de treinamento tamb√©m.
 - Essa combina√ß√£o de termos completos e fragmentos (stemmer + sufixo) possibilita criar palavras por combina√ß√£o ao submeter um documento novo ao modelo que contenha termos fora do vocam de treinamento. Pode ser interessante manter o stemmer apenas dos termos mais relevantes para o contexto do treinamento.
 - Opcionalmente pode-se usar o par√¢metro `-treino` para gerar o arquivo de curadoria com base nos arquivos da pasta `texto_treino`, ou o par√¢metro `-teste` para os arquivos da pasta `texto_teste`. O par√¢metro `-vocab_treino` pode ser √∫til para comparar os termos dos textos com os termos realmente treinados no vocab ap√≥s o in√≠cio do treinamento. Pode-se combinar os par√¢metros `-treino -vocab_treino` ou `-teste -vocab_treino`. Com isso √© poss√≠vel avaliar rapidamente o comportamento do vocabul√°rio selecionado para tokeniza√ß√£o at√© o vocabul√°rio utilizado pelo `Gensim` no treinamento.
> :bulb: <sub> Nota: √© importante ressaltar que ap√≥s modelo ser gerado, apenas os termos contidos no vocabul√°rio do modelo ser√£o usados na vetoriza√ß√£o. Os termos realmente treinados podem ser visualizados no arquivo `vocab_treino.txt` gerado ap√≥s a primeira √©poca de treinamento.</sub>  
   
 - <b>Exemplo</b>: `engloba` pode ser composta por `englob` `#a`, e outras forma√ß√µes podem ocorrer como `englob` `#ada`, `englob` `#adamente`, `englob` `#adas` caso esses  fragmentos estejam dispon√≠veis no vocabul√°rio de treinamento.
   - O vocab de treinamento n√£o precisa do `#` antes do sufixo, apenas dos fragmentos. Mas durante o treinamento os fragmentos usados como sufixo iniciar√£o com `#` para facilitar sua identifica√ß√£o e diferenciar dos termos principais no modelo final.
 - <b>Exemplo de tokeniza√ß√£o com termos simples, compostos e fragmentos</b>: 
   ```
   ['atendiam_testemunha', 'seu', 'depoimento', 'apesar', 'de', 'trazer', 'algumas', 'impreciso', '#es', 'sobre', 'os', 'fatos', 'atend', '#o', 'se', 'os', 'jurados', 'as', 'provas', 'produzidas', 'em', 'plenari', '#os']
   ```

- Veja o [`passo a passo`](./docs/passo_a_passo_facil.md) para criar o vocabul√°rio de treinamento de acordo com o cen√°rio desejado e realizar o treinamento propriamente dito.

### Exemplo de arquivo `curadoria_planilha_vocab.xlsx` de curadoria de termos:
![recorte curadoria_planilha_vocab.xlsx](./exemplos/img_corte_plan_curadoria.png?raw=true "Recorte planilha curadoria")

> üí° Notas sobre as colunas: 
> - `TFIDF` - cont√©m a m√©dia dos pesos que o termo teve nos documentos - [Saiba mais sobre `TFIDF`](https://www.ti-enxame.com/pt/python/interpretar-um-resumo-das-pontuacoes-das-palavras-do-tf-idf-nos-documentos/829990829/), ou [`aqui`](https://iyzico.engineering/how-to-calculate-tf-idf-term-frequency-inverse-document-frequency-from-the-beatles-biography-in-c4c3cd968296).
> - `TAMANHO` - √© o tamanho do termo
> - `QTD` - √© a quantidade de vezes que o termo apareceu no corpus
> - `QTD_DOCS` - √© a quantidade de documentos onde o termo apareceu
> - `COMPOSTO` Sim / N√£o - indica se o termo √© composto 
> - `VOCAB` - indica se o termo est√° presente no vocab principal, se est√° presente quando fragmentado, se √© composto, se √© removido do vocab ou se n√£o est√° presente no vocab.
> - `ESTRANHO` Sim / N√£o - termos sem vogais ou com consoantes/vogais com v√°rias repeti√ß√µes
> - Ao final s√£o inclu√≠das algumas colunas indicando a posi√ß√£o das colunas `TFIDF`, `TAMANHO`, `QTD` e `QTD_DOCS` no BoxPlot de cada coluna.

## Conferindo o processamento dos textos
- Durante o treinamento s√£o criados arquivos com a extens√£o `.clr` para cada arquivo `.txt` da pasta `texto_treino`, eles s√£o o resultado do processamento dos textos originais com o `TokenizadorInteligente`.
- Nesses arquivos √© poss√≠vel identificar os fragmentos, os tokens principais e os termos compostos, e verificar se a tokeniza√ß√£o est√° de acordo com o esperado. O treinamento do modelo ser√° feito com esse arquivo e o vocab de treinamento √© o resultado da an√°lise desses termos. O objetivo do `TokenizadorInteligente` √© preparar o texto que ser√° treinado e fazer o mesmo processo em textos novos para vetoriza√ß√£o usando o modelo no futuro. 
- No in√≠cio do treinamento os arquivos `.clr` ser√£o atualizados para garantir que novos termos inclu√≠dos ou alterados manualmente sejam refletidos na tokeniza√ß√£o.
- Os arquivos `.clr` s√£o necess√°rios durante todo o treinamento e ser√£o recriados se n√£o forem encontrados, isso acelera o treinamento para n√£o haver necessidade de reprocessar os textos cada vez que o treinamento passar por eles.

## Treinando o modelo doc2vec: 
 Com os arquivos de vocab prontos, criados automaticamente ou manualmente, pode-se treinar o modelo.<br>
 Siga os passos do cen√°rio que atende √† sua necessidade: [`passo a passo`](./docs/passo_a_passo_facil.md)

### Par√¢metros
 - `python util_doc2vec_facil.py`
    - `-pasta` - nome da pasta de treinamento que cont√©m a pasta do modelo e as pastas de textos, o padr√£o √© `meu_modelo` se n√£o for informada.
    - `-treinar`' - iniciar o treinamento do modelo
    - `-reiniciar sim` - remove o modeo atual, se existir, e inicia um novo treinamento (o sim √© para garantir que se quer apagar o modelo e reiniciar). Pode-se tamb√©m apagar manualmente os arquivos `doc2vec.*` para reiniciar o modelo.
    - `-testar` - carrega o modelo atual, se existir, e apresenta no console a compara√ß√£o de termos encontrados no arquivo `termos_comparacao_treino.txt` e o agrupamento de textos da pasta `textos_teste`.
    - `-epocas` - define o n√∫mero de √©pocas que ser√£o treinadas, o padr√£o √© 5000 e pode ser interrompido ou acrescido a qualquer momento.
    - `-dimensoes` - define o n√∫mero de dimens√µes dos vetores de treinamento (n√£o pode ser alterado depois de iniciado o treinamento). N√£o sabe como escolher use o padr√£o 300.
    - `-workers` - n√∫mero de threads de treinamento, padr√£o 100

 - `python util_doc2vec_vocab_facil.py`
    - `-pasta` - nome da pasta de treinamento que cont√©m as pastas de textos, o padr√£o √© `meu_modelo` se n√£o for informada.
    - `-treino` - cria a planilha de curadoria com a pasta `textos_treino`, sem esse par√¢mtro a planilha √© criada com a pasta `textos_vocab`.
    - `-teste` - cria a planilha de curadoria com a pasta `textos_teste`, sem esse par√¢mtro a planilha √© criada com a pasta `textos_vocab`.
  > :bulb: <sub>Nota: s√£o muitas op√ß√µes de uso da curadoria para cria√ß√£o de um bom vocab. Se n√£o sabe por onde come√ßar, copie todos os termos listados na planilha para um arquivo `VOCAB_BASE.txt` e crie um arquivo `VOCAB_REMOVIDO.txt` com stopwords.</sub><br>
  <sub>Caso os seus documentos sejam limpos (n√£o sejam ocerizados), preocupe-se inicialmente apenas com o arquivo `VOCAB_REMOVIDO.txt` para um treino mais simples sem prefixos e sufixos treinando todo o vocabul√°rio e removendo apenas stopwords.</sub>
  
### Termos comparados para acompanhar a evolu√ß√£o do modelo:
- Exemplo de sa√≠da do arquivo `comparar_termos.log` atualizado a cada √©poca.
- Esse log √© gerado com os termos ou frases dispon√≠veis no arquivo `termos_comparacao_treino.txt` que √© carregado no in√≠cio do treino e pode ser alterado sempre que desejado.
  - o arquivo cont√©m termos linha a linha e frases que podem ser comparadas.
Exemplo do arquivo `termos_comparacao_treino.txt`:
```
apresentada para o r√©u a decis√£o sobre o processo = apresentada para o acusado a senten√ßa sobre o processo
artigo
cobrados
cogitar 
compromisso
comprovadas
entendimento
lei
entorpecentes
julga
parcelas
termo
```
> üí° <sub>Nota: na primeira linha temos duas frases que ser√£o comparadas ao longo do treino. Nas outras linhas temos termos soltos que ser√£o apresentados os termos mais parecidos durante o treino. Coloque quantos termos ou frases desejar. Aparecer√£o os termos que tiverem similares com mais de 50% de similaridade.</sub><br>
  > <sub>O resultado do arquivo `comparar_termos.log` √© esse:</sub>
```
apresentada para o r√©u a decis√£o sobre o processo | apresentada para o acusado a senten√ßa sobre o processo (65%)
artigo               |  art (77%)                |  artigos (55%)            |  arts (53%)              
cobrados             |  pagos (54%)             
cogitar              |  falar (52%)             
compromisso          |  promessa (57%)          
comprovadas          |  demonstradas (58%)      
entendimento         |  posicionamento (52%)    
lei                  |  lei_federal (75%)        |  lei_complementar (72%)   |  cpp (63%) 
entorpecentes        |  drogas (64%)            
julga                |  julgou (71%)             |  julgando (53%)          
parcelas             |  prestacoes (60%)         |  despesas (55%)           |  quantias (52%)          
termo                |  peticao (64%)            |  inepcia (63%)           
```

### Arquivos comparados para acompanhar a evolu√ß√£o do modelo:
- Semelhante a compara√ß√£o de termos, pode-se criar uma pasta `textos_teste` com alguns arquivos para compara√ß√£o durante o treinamento. 
- Uma sugest√£o de avalia√ß√£o do modelo √© colocar arquivos com indicadores de grupos para avaliar pelos nomes se est√£o sendo agrupados como esperado.
- Por exemplo, colocando no nome do arquivo `grupo1`, `grupo2` etc, pode-se avaliar se os arquivos est√£o sendo agrupados por similaridade da forma que √© desejado. Com isso pode-se ajustar os par√¢metros do modelo para avaliar se est√° se aproximando mais ou menos do que se espera. 
- O resultado de compara√ß√£o ser√° colocado no arquivo `comparar_arquivos.log` como no exemplo abaixo.
- Um dos arquivos √© escolhido para ser comparado com ele mesmo para ter mais uma informa√ß√£o para avalia√ß√£o do modelo. Como informado na documenta√ß√£o do Doc2Vec, chamadas subsequentes de um mesmo conte√∫do podem inferir vetores diferentes. Essa diferen√ßa diminui aumentando o n√∫mero de √©pocas na infer√™ncia do vetor. 
- Como a compara√ß√£o de arquivos pode exigir mais processamento, ela aguarda pelo menos 5 minutos para ser atualizada. Se o arquivo for exclu√≠do, ela ser√° realizada ap√≥s o final da pr√≥xima √©poca. A compara√ß√£o de arquivos tamb√©m ser√° realizada ao usar o par√¢metro `-testar` para testar o modelo.
![exemplo arquivo comparar_arquivos.log](./exemplos/img_comparar_arquivos.png?raw=true "comparar arquivos.log")
> :bulb: <sub>Nota: Essa rotula√ß√£o n√£o faz o treinamento ser supervisionado, apenas auxilia a avalia√ß√£o do modelo, j√° que os r√≥tulos n√£o s√£o levados em considera√ß√£o no treinamento.</sub>

## Usando o modelo:
O que precisa ser disponibilizado para o modelo funcionar:<br>
 :file_folder: `modelo_teste` <br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - `VOCAB_BASE*.txt` - arquivo com termos e fragmentos que comp√µem o vocab <br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - `VOCAB_TRADUTOR*.txt` - arquivo de transforma√ß√µes do tokenizados <br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - `VOCAB_REMOVER*.txt` - arquivo de exclus√µes do tokenizados (stopwords por exemplo) <br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - `doc2vec*` - arquivos do modelo treinado <br>
 
 Coloque tudo em uma pasta com o nome que desejar e pronto. Esse √© um pacote do seu modelo. <br>

O modelo pode ser carregado facilmente:
 ```python 
 from util_doc2vec_facil import UtilDoc2VecFacil
 dv = UtilDoc2VecFacil(pasta_modelo=PASTA_MODELO)

frase1 = 'EXECU√á√ÉO POR T√çTULO EXTRAJUDICIAL DE HONOR√ÅRIO ADVOCAT√çCIO EMBARGOS ADJUDICA√á√ÉO PENHORAS'
frase2 = 'EMENTA SEGUROs de VIDA COBRAN√áA CUMULADA C PRETENS√ÉO INDENIZAT√ìRIA PRESCRI√á√ÉO RECONHECIDA'
frase3 = 'DE HONOR√ÅRIOS ADVOCAT√çCIOS EMBARGOS ADJUDICA√á√ÉO PENHORA EXECU√á√ÉO POR T√çTULO EXTRAJUDICIAL '
print('Frase1: ', frase1, '\nFrase2: ', frase2, '\n\t - Similaridade: ', dv.similaridade(frase1,frase2))
print('Frase1: ', frase1, '\nFrase3: ', frase3, '\n\t - Similaridade: ', dv.similaridade(frase1,frase3))        
print('\nTokens frase 1: ', dv.tokens_sentenca(frase1))
 ```
 
 <b>Resultado:</b>
 ```
Frase1:  EXECU√á√ÉO POR T√çTULO EXTRAJUDICIAL DE HONOR√ÅRIO ADVOCAT√çCIO EMBARGOS ADJUDICA√á√ÉO PENHORAS
Frase2:  EMENTA SEGUROs de VIDA COBRAN√áA CUMULADA C PRETENS√ÉO INDENIZAT√ìRIA PRESCRI√á√ÉO RECONHECIDA
         - Similaridade:  0.43190062046051025
Frase1:  EXECU√á√ÉO POR T√çTULO EXTRAJUDICIAL DE HONOR√ÅRIO ADVOCAT√çCIO EMBARGOS ADJUDICA√á√ÉO PENHORAS
Frase3:  DE HONOR√ÅRIOS ADVOCAT√çCIOS EMBARGOS ADJUDICA√á√ÉO PENHORA EXECU√á√ÉO POR T√çTULO EXTRAJUDICIAL
         - Similaridade:  0.46588313579559326

Tokens frase 1:  ['execucao', 'por', 'titulo', 'extrajudicial', 'de', 'honorario', 'advocaticio', 'embargos', 'adjudicacao', 'penhoras'] 
 ```
 
## Dicas de uso: <a name="dicas">
- gravar os vetores, textos e metadados dos documentos no [`ElasticSearch`](https://www.elastic.co/pt/), e usar os recursos de pesquisas: More Like This, vetoriais e por proximidade de termos como disponibilizado no componente [`PesquisaElasticFacil`](https://github.com/luizanisio/PesquisaElasticFacil).
- gravar os vetores, textos e metadados no [`SingleStore`](https://www.singlestore.com/) e criar views de similaridade para consulta em tempo real dos documentos inseridos na base, incluindo filtros de metadados e textuais como nos exemplos dispon√≠veis aqui: [`dicas SingleStore`](./docs/readme_dicas.md).
